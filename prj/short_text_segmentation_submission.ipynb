{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk as nltk\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# knowledge base available at http://probase.msra.cn/dataset.aspx\n",
    "concepts = pd.read_csv('data/concepts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentences=['april in paris lyrics',\n",
    "                'april in paris vacation',\n",
    "               'hotel california eagles',\n",
    "                'read harry potter',\n",
    "                'read harry potter book',\n",
    "                'watch harry potter movie',\n",
    "                'manchester city beat manchester united and won the trophy',\n",
    "                'niagara falls best season to visit',\n",
    "                'how to hone randomized algorithms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creation of hash index of words from knowledge base to speed up word search\n",
    "def create_hash_index(knowledgebase):\n",
    "    hash_index = {}\n",
    "    for inst in knowledgebase['instance'].unique():\n",
    "        hash_index[inst] = 1\n",
    "    return hash_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting of the sentence in all possible ways\n",
    "# source https://stackoverflow.com/questions/18406776/split-a-string-into-all-possible-ordered-phrases\n",
    "def break_down(text):\n",
    "    words = text.split()\n",
    "    ns = range(1, len(words)) # n = 1..(n-1)\n",
    "    for n in ns[::-1]: # split into 2, 3, 4, ..., n parts.\n",
    "        for idxs in itertools.combinations(ns, n):\n",
    "            yield [' '.join(words[i:j]) for i, j in zip((0,) + idxs, idxs + (None,))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculation of the affinity of two words\n",
    "def edge_weight(concepts, w1, w2):\n",
    "    w1_vec = concepts[concepts.instance == w1][['concept', 'relations']]\n",
    "    w2_vec = concepts[concepts.instance == w2][['concept', 'relations']]  \n",
    "    intersection = pd.merge(w1_vec, w2_vec, on='concept')\n",
    "    return 1+intersection['relations_x'].dot(intersection['relations_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# composition of term graph\n",
    "def construct_graph(string, h_index, stopwrds, knowledgebase):\n",
    "    nodes_to_words = {}\n",
    "    words_to_nodes = {}\n",
    "    graph = np.empty((1,3), dtype=int)\n",
    "    i = 0\n",
    "    for split in break_down(string):\n",
    "        reduced = []\n",
    "        for w in split: \n",
    "            if w not in h_index:\n",
    "                reduced = []\n",
    "                break\n",
    "            if w in stopwrds:\n",
    "                continue\n",
    "            reduced += [w,]\n",
    "\n",
    "        new_nodes = []    \n",
    "        for j,w in enumerate(reduced):\n",
    "            if w not in words_to_nodes:\n",
    "                nodes_to_words[i] = w\n",
    "                words_to_nodes[w] = i\n",
    "                for k in range(j):\n",
    "                    dest_w = reduced[k]\n",
    "                    dest_v = words_to_nodes[dest_w]\n",
    "                    weight = edge_weight(knowledgebase, w, dest_v)\n",
    "                    new_edge = np.array([[i, dest_v, weight]])\n",
    "                    graph = np.append(graph, new_edge, axis=0)\n",
    "                new_nodes += [i,]\n",
    "                i += 1\n",
    "            else:\n",
    "                for u in new_nodes:\n",
    "                    dest_v = words_to_nodes[w]\n",
    "                    u_w = nodes_to_words[u]\n",
    "                    weight = edge_weight(knowledgebase, w, u_w)\n",
    "                    new_edge = np.array([[dest_v, u, weight]])\n",
    "                    graph = np.append(graph, new_edge, axis=0)\n",
    "    return graph[1:], nodes_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random selection of an edge proportionally to its weight\n",
    "def rnd_edg_sel(graph):\n",
    "    rng = graph[:,2].sum()\n",
    "    rnd_pt = np.random.random()*rng\n",
    "    cml_sum=0\n",
    "    for edge in graph:\n",
    "        cml_sum += edge[2]\n",
    "        if cml_sum>rnd_pt:\n",
    "            return edge[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_edge(graph, edge):\n",
    "    new_graph = np.empty((1, graph.shape[1]), dtype=int)\n",
    "    for e in graph:\n",
    "        ed = e[np.newaxis,:]\n",
    "        if (ed == edge).all():\n",
    "            continue\n",
    "        new_graph = np.append(new_graph, ed, axis=0)\n",
    "    return new_graph[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove edges related to a particular vertex\n",
    "def remove_rel_edges(graph, t):\n",
    "    new_graph = np.empty((1, graph.shape[1]), dtype=int)\n",
    "    for e in graph:\n",
    "        ed = e[np.newaxis,:]\n",
    "        if e[0] == t or e[1] == t:\n",
    "            continue\n",
    "        new_graph = np.append(new_graph, ed, axis=0)\n",
    "    return new_graph[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def are_connected(graph, u, v):\n",
    "    for e in graph:\n",
    "        if u in e[:2] and v in e[:2]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomised algorithm for maximal clique calculation\n",
    "def MaxCMC(graph):\n",
    "    p_graph = np.empty((1,graph.shape[1]), dtype=int)\n",
    "    removed_v = np.array([])\n",
    "    while graph.size > 0:\n",
    "        e = rnd_edg_sel(graph)\n",
    "#         print('selected e =', e)\n",
    "        u = e[0,0]\n",
    "        v = e[0,1]\n",
    "        removed_v = np.append(removed_v, np.array([u,v]))\n",
    "        p_graph = np.append(p_graph, e, axis=0)\n",
    "        graph = remove_edge(graph, e)\n",
    "        for t in graph[:,:2].flatten():\n",
    "            if t in removed_v: #t==u or t==v:\n",
    "                continue\n",
    "            if not are_connected(graph, t, v) or not are_connected(graph, t, u):\n",
    "                graph = remove_rel_edges(graph, t)\n",
    "    return p_graph[1:], p_graph[1:,2].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# repetition of the randomised clique calculation k times for better precision\n",
    "def CMaxC(graph, nodes_to_words, k=3):\n",
    "    max_w = 0\n",
    "    for i in range(k):\n",
    "        clique, weight = MaxCMC(graph)\n",
    "        if weight>max_w:\n",
    "            best_cl = clique\n",
    "            max_w = weight\n",
    "    word_nos = np.sort(np.unique(best_cl[:,:2]))\n",
    "    result = [nodes_to_words[no] for no in word_nos]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segment_sentence(knowledgebase, sentence, stpwrds):\n",
    "    hash_index = create_hash_index(knowledgebase)\n",
    "    graph, ntw = construct_graph(sentence, hash_index, stpwrds, knowledgebase)\n",
    "    return CMaxC(graph, ntw)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'april in paris lyrics'"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['april', 'paris', 'lyrics']\n",
      "['vacation', 'april in paris']\n",
      "['eagles', 'hotel california']\n",
      "['read', 'harry', 'potter']\n",
      "['read', 'book', 'harry potter']\n",
      "['watch', 'movie', 'harry potter']\n",
      "['manchester', 'city', 'beat', 'trophy', 'manchester united']\n",
      "['best', 'season', 'visit', 'niagara falls']\n",
      "['hone', 'randomized', 'algorithms', 'how to']\n"
     ]
    }
   ],
   "source": [
    "for tstr in test_sentences:\n",
    "    segmentation = segment_sentence(concepts, tstr, swords)\n",
    "    print(segmentation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
